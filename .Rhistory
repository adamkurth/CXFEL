# Create histograms of the coefficients for different lambda values
library(ggplot2)
library(dplyr)
coef_df_long <- coef_df %>%
pivot_longer(-lambda, names_to = "Variable", values_to = "Coefficient")
ggplot(coef_df_long, aes(x = Coefficient)) +
geom_histogram(binwidth = 0.2) +
facet_wrap(~Variable, scales = "free_y") +
labs(x = "Coefficient Value", y = "Frequency") +
ggtitle("Distribution of Ridge Regression Coefficients for Different Lambda Values")
# Fit a Ridge Regression model with a sequence of lambda values
lambda_seq <- 10^seq(-2, 4, length = 100)
ridge_model <- glmnet(X_train, Y_train, alpha = 0, lambda = lambda_seq)
# Extract and transpose the coefficients and convert to a data frame
coef_matrix <- as.matrix(coef(ridge_model))
coef_df <- data.frame(lambda = log(lambda_seq), t(coef_matrix))
# Create histograms of the coefficients for different lambda values
library(ggplot2)
library(dplyr)
coef_df_long <- coef_df %>%
pivot_longer(-lambda, names_to = "Variable", values_to = "Coefficient")
ggplot(coef_df_long, aes(x = Coefficient)) +
geom_histogram(binwidth = 0.2) +
facet_wrap(~Variable, scales = "free_y") +
labs(x = "Coefficient Value", y = "Frequency") +
ggtitle("Distribution of Ridge Regression Coefficients for Different Lambda Values")
more-lambda = ggplot(coef_df_long, aes(x = Coefficient)) +
geom_histogram(binwidth = 0.2) +
facet_wrap(~Variable, scales = "free_y") +
labs(x = "Coefficient Value", y = "Frequency") +
ggtitle("Distribution of Ridge Regression Coefficients for Different Lambda Values")
more_lambda = ggplot(coef_df_long, aes(x = Coefficient)) +
geom_histogram(binwidth = 0.2) +
facet_wrap(~Variable, scales = "free_y") +
labs(x = "Coefficient Value", y = "Frequency") +
ggtitle("Distribution of Ridge Regression Coefficients for Different Lambda Values")
more_lambda
# Calculate residuals for different lambda values
lambda_seq <- 10^seq(-2, 4, length = 100)
residuals <- vector("list", length(lambda_seq))
for (i in seq_along(lambda_seq)) {
lambda <- lambda_seq[i]
ridge_model <- glmnet(X_train, Y_train, alpha = 0, lambda = lambda)
Y_pred_train <- predict(ridge_model, newx = X_train)
residuals[[i]] <- Y_train - Y_pred_train
}
# Create a data frame for the residuals
residual_df <- data.frame(Lambda = log(lambda_seq), Residual = unlist(residuals))
# Create a histogram or density plot of residuals for different lambda values
library(ggplot2)
residual_plot <- ggplot(residual_df, aes(x = Residual, fill = Lambda)) +
geom_histogram(binwidth = 0.2, position = "identity") +
scale_fill_gradient(low = "blue", high = "red") +
labs(
x = "Residual",
y = "Frequency",
fill = "Log Lambda (Regularization Strength)"
) +
theme_minimal()
residual_plot
residual_plot
# Make predictions on the training set
Y_pred_train <- predict(ridge_model, newx = X_train)
pred_df_train <- data.frame(actual = Y_train, predicted = Y_pred_train)
# Make predictions on the training set
Y_pred_train <- predict(ridge_model, newx = X_train)
# Create a data frame with the actual and predicted values
pred_df_train <- data.frame(actual = Y_train, predicted = Y_pred_train)
# display the actual versus predicted values
pred_plot <- ggplot(pred_df_train, aes(x = Y_train, y = Y_pred_train)) + geom_point() + geom_smooth(method = "lm", color = "red", se = FALSE) + labs(x = "Actual Value", y = "Predicted Value") + theme_minimal()
pred_plot
# Load the GGally package
library(GGally)
predictor_df <- data.frame(X_train); predictor_df$Y_train <- Y_train
# Create a scatter plot matrix
scatter_plot <- ggpairs(predictor_df, columns = 1:seq(ncol(predictor_df)), aes(color = Y_train))
# Create a subplot with the two plots
subplot(scatter_plot, pred_plot, nrows = 2)
It looks like you want to produce a plot of actual versus predicted values along with a linear regression line. To do this for the residuals as lambda varies, you can adapt the previous code as follows:
# Calculate residuals for different lambda values
lambda_seq <- 10^seq(-2, 4, length = 100)
residuals <- vector("list", length(lambda_seq))
for (i in seq_along(lambda_seq)) {
lambda <- lambda_seq[i]
ridge_model <- glmnet(X_train, Y_train, alpha = 0, lambda = lambda)
Y_pred_train <- predict(ridge_model, newx = X_train)
residuals[[i]] <- Y_train - Y_pred_train
}
# Create a data frame for the residuals
residual_df <- data.frame(Lambda = log(lambda_seq), Residual = unlist(residuals))
# Create a plot of actual versus predicted values along with a linear regression line
library(ggplot2)
pred_plot <- ggplot(residual_df, aes(x = Residual)) +
geom_point() +
geom_smooth(method = "lm", formula = y ~ x, color = "red", se = FALSE) +
labs(x = "Actual Value", y = "Predicted Value") +
theme_minimal()
pred_plot
library(plotly)
library(glmnet)
library(caret)
library(Metrics)
library(ISLR)
library(tidyr)
# Generate a synthetic dataset with multicollinearity and added noise
set.seed(123)
n <- 200
x1 <- rnorm(n)
x2 <- 2 * x1 + rnorm(n)
x3 <- -x1 + rnorm(n)
y <- 2 * x1 + 3 * x2 - x3 + rnorm(n)
# Create a data frame of predictors
housing_data <- data.frame(X1 = x1, X2 = x2, X3 = x3, Y = y)
# Standardize the predictor variables
X_train <- scale(housing_data[, -4])
Y_train <- housing_data$Y
# Fit a Ridge Regression model with a sequence of lambda values
lambda_seq <- 10^seq(-2, 4, length = 100)
ridge_model <- glmnet(X_train, Y_train, alpha = 0, lambda = lambda_seq)
# Extract and transpose the coefficients and convert to a data frame
coef_matrix <- as.matrix(coef(ridge_model))
coef_df <- data.frame(lambda = log(lambda_seq), t(coef_matrix))
# Create a list to store traces
traces <- list()
# Loop through each predictor variable and create a trace for each
for (i in seq(nrow(coef_matrix))) {
if (i + 1 > ncol(coef_df)) {
stop("Error: Column index out of bounds.")
}
trace <- list(
x = coef_df$lambda,
y = coef_df[, i + 1],
type = "scatter",
mode = "lines",
name = colnames(coef_matrix)[i]
)
traces[[i]] <- trace
}
# Create the plot
plot_ly(data = coef_df, x = ~lambda) %>%
add_trace(x = ~lambda, y = ~coef_df[,2], type = "scatter", mode = "lines", name = colnames(coef_df)[2]) %>%
add_trace(x = ~lambda, y = ~coef_df[,3], type = "scatter", mode = "lines", name = colnames(coef_df)[3]) %>%
add_trace(x = ~lambda, y = ~coef_df[,4], type = "scatter", mode = "lines", name = colnames(coef_df)[4]) %>%
# Add more traces for other coefficients as needed
layout(
title = "Ridge Regression Coefficients vs. Log Lambda",
xaxis = list(title = "Log Lambda (Regularization Strength)"),
yaxis = list(title = "Coefficient Value")
) # Display the plot
# Create a data frame to store the condition number for each lambda value
cond_df <- data.frame(lambda = lambda_seq, cond_num = NA)
# Loop through each lambda value and calculate the condition number
for (i in seq_along(lambda_seq)) {
lambda <- lambda_seq[i]
ridge_model <- glmnet(X_train, Y_train, alpha = 0, lambda = lambda)
X_mat <- as.matrix(X_train)  # Use the standardized predictor matrix directly
cond_num <- max(svd(X_mat)$d) / min(svd(X_mat)$d)
cond_df[i, "cond_num"] <- cond_num
}
# display the condition number
cond = ggplot(cond_df, aes(x = lambda, y = cond_num)) +
geom_line() + geom_point() +
labs(x = "Lambda", y = "Condition Number") + theme_minimal()
plot_ly(data = coef_df, x = ~lambda) %>%
add_trace(x = ~lambda, y = ~coef_df[,2], type = "scatter", mode = "lines", name = colnames(coef_df)[2]) %>%
add_trace(x = ~lambda, y = ~coef_df[,3], type = "scatter", mode = "lines", name = colnames(coef_df)[3]) %>%
add_trace(x = ~lambda, y = ~coef_df[,4], type = "scatter", mode = "lines", name = colnames(coef_df)[4]) %>%
layout(title = "Ridge Regression Coefficients vs. Log Lambda",
xaxis = list(title = "Log Lambda (Regularization Strength)"),
yaxis = list(title = "Coefficient Value")
) %>% subplot( ggplotly( ggplot(cond_df, aes(x = lambda, y = cond_num)) +geom_line() + geom_point() + labs(x = "Lambda", y = "Condition Number") + theme_minimal() ), nrows = 2)
# X - axis = lambda conveys the condition number.
# Make predictions on the training set
Y_pred_train <- predict(ridge_model, newx = X_train)
pred_df_train <- data.frame(actual = Y_train, predicted = Y_pred_train)
# Make predictions on the training set
Y_pred_train <- predict(ridge_model, newx = X_train)
# Create a data frame with the actual and predicted values
pred_df_train <- data.frame(actual = Y_train, predicted = Y_pred_train)
# display the actual versus predicted values
pred_plot <- ggplot(pred_df_train, aes(x = Y_train, y = Y_pred_train)) + geom_point() + geom_smooth(method = "lm", color = "red", se = FALSE) + labs(x = "Actual Value", y = "Predicted Value") + theme_minimal()
pred_plot
# Load the GGally package
library(GGally)
predictor_df <- data.frame(X_train); predictor_df$Y_train <- Y_train
# Create a scatter plot matrix
scatter_plot <- ggpairs(predictor_df, columns = 1:seq(ncol(predictor_df)), aes(color = Y_train))
# Create a subplot with the two plots
subplot(scatter_plot, pred_plot, nrows = 2)
residuals <- vector("list", length(lambda_seq))
for (i in seq_along(lambda_seq)) {
lambda <- lambda_seq[i]
ridge_model <- glmnet(X_train, Y_train, alpha = 0, lambda = lambda)
Y_pred_train <- predict(ridge_model, newx = X_train)
residuals[[i]] <- Y_train - Y_pred_train
}
# Create a data frame for the residuals
residual_df <- data.frame(Lambda = log(lambda_seq), Residual = unlist(residuals))
# Create a plot of actual versus predicted values along with a linear regression line
library(ggplot2)
pred_plot <- ggplot(residual_df, aes(x = Residual)) +
geom_point() +
geom_smooth(method = "lm", formula = y ~ x, color = "red", se = FALSE) +
labs(x = "Actual Value", y = "Predicted Value") +
theme_minimal()
pred_plot
# Load the GGally package
library(GGally)
predictor_df <- data.frame(X_train); predictor_df$Y_train <- Y_train
# Create a scatter plot matrix
scatter_plot <- ggpairs(predictor_df, columns = 1:seq(ncol(predictor_df)), aes(color = Y_train))
# Create a subplot with the two plots
subplot(scatter_plot, pred_plot, nrows = 2)
# Load the GGally package
library(GGally)
predictor_df <- data.frame(X_train); predictor_df$Y_train <- Y_train
# Create a scatter plot matrix
scatter_plot <- ggpairs(predictor_df, columns = 1:seq(ncol(predictor_df)), aes(color = Y_train))
# Create a subplot with the two plots
subplot(scatter_plot, pred_plot, nrows = 2)
library(plotly)
library(glmnet)
library(caret)
library(Metrics)
library(ISLR)
library(tidyr)
# Generate a synthetic dataset with multicollinearity and added noise
set.seed(123)
n <- 200
x1 <- rnorm(n)
x2 <- 2 * x1 + rnorm(n)
x3 <- -x1 + rnorm(n)
y <- 2 * x1 + 3 * x2 - x3 + rnorm(n)
# Create a data frame of predictors
housing_data <- data.frame(X1 = x1, X2 = x2, X3 = x3, Y = y)
# Standardize the predictor variables
X_train <- scale(housing_data[, -4])
Y_train <- housing_data$Y
# Fit a Ridge Regression model with a sequence of lambda values
lambda_seq <- 10^seq(-2, 4, length = 100)
ridge_model <- glmnet(X_train, Y_train, alpha = 0, lambda = lambda_seq)
# Extract and transpose the coefficients and convert to a data frame
coef_matrix <- as.matrix(coef(ridge_model))
coef_df <- data.frame(lambda = log(lambda_seq), t(coef_matrix))
# Create a list to store traces
traces <- list()
# Loop through each predictor variable and create a trace for each
for (i in seq(nrow(coef_matrix))) {
if (i + 1 > ncol(coef_df)) {
stop("Error: Column index out of bounds.")
}
trace <- list(
x = coef_df$lambda,
y = coef_df[, i + 1],
type = "scatter",
mode = "lines",
name = colnames(coef_matrix)[i]
)
traces[[i]] <- trace
}
# Create the plot
plot_ly(data = coef_df, x = ~lambda) %>%
add_trace(x = ~lambda, y = ~coef_df[,2], type = "scatter", mode = "lines", name = colnames(coef_df)[2]) %>%
add_trace(x = ~lambda, y = ~coef_df[,3], type = "scatter", mode = "lines", name = colnames(coef_df)[3]) %>%
add_trace(x = ~lambda, y = ~coef_df[,4], type = "scatter", mode = "lines", name = colnames(coef_df)[4]) %>%
# Add more traces for other coefficients as needed
layout(
title = "Ridge Regression Coefficients vs. Log Lambda",
xaxis = list(title = "Log Lambda (Regularization Strength)"),
yaxis = list(title = "Coefficient Value")
) # Display the plot
# Create a data frame to store the condition number for each lambda value
cond_df <- data.frame(lambda = lambda_seq, cond_num = NA)
# Loop through each lambda value and calculate the condition number
for (i in seq_along(lambda_seq)) {
lambda <- lambda_seq[i]
ridge_model <- glmnet(X_train, Y_train, alpha = 0, lambda = lambda)
X_mat <- as.matrix(X_train)  # Use the standardized predictor matrix directly
cond_num <- max(svd(X_mat)$d) / min(svd(X_mat)$d)
cond_df[i, "cond_num"] <- cond_num
}
# display the condition number
cond = ggplot(cond_df, aes(x = lambda, y = cond_num)) +
geom_line() + geom_point() +
labs(x = "Lambda", y = "Condition Number") + theme_minimal()
plot_ly(data = coef_df, x = ~lambda) %>%
add_trace(x = ~lambda, y = ~coef_df[,2], type = "scatter", mode = "lines", name = colnames(coef_df)[2]) %>%
add_trace(x = ~lambda, y = ~coef_df[,3], type = "scatter", mode = "lines", name = colnames(coef_df)[3]) %>%
add_trace(x = ~lambda, y = ~coef_df[,4], type = "scatter", mode = "lines", name = colnames(coef_df)[4]) %>%
layout(title = "Ridge Regression Coefficients vs. Log Lambda",
xaxis = list(title = "Log Lambda (Regularization Strength)"),
yaxis = list(title = "Coefficient Value")
) %>% subplot( ggplotly( ggplot(cond_df, aes(x = lambda, y = cond_num)) +geom_line() + geom_point() + labs(x = "Lambda", y = "Condition Number") + theme_minimal() ), nrows = 2)
# X - axis = lambda conveys the condition number.
# Make predictions on the training set
Y_pred_train <- predict(ridge_model, newx = X_train)
pred_df_train <- data.frame(actual = Y_train, predicted = Y_pred_train)
# Make predictions on the training set
Y_pred_train <- predict(ridge_model, newx = X_train)
# Create a data frame with the actual and predicted values
pred_df_train <- data.frame(actual = Y_train, predicted = Y_pred_train)
# display the actual versus predicted values
pred_plot <- ggplot(pred_df_train, aes(x = Y_train, y = Y_pred_train)) + geom_point() + geom_smooth(method = "lm", color = "red", se = FALSE) + labs(x = "Actual Value", y = "Predicted Value") + theme_minimal()
pred_plot
# Load the GGally package
library(GGally)
predictor_df <- data.frame(X_train); predictor_df$Y_train <- Y_train
# Create a scatter plot matrix
scatter_plot <- ggpairs(predictor_df, columns = 1:seq(ncol(predictor_df)), aes(color = Y_train))
# Create a subplot with the two plots
subplot(scatter_plot, pred_plot, nrows = 2)
# Calculate residuals for different lambda values
lambda_seq <- 10^seq(-2, 4, length = 100)
residuals <- vector("list", length(lambda_seq))
for (i in seq_along(lambda_seq)) {
lambda <- lambda_seq[i]
ridge_model <- glmnet(X_train, Y_train, alpha = 0, lambda = lambda)
Y_pred_train <- predict(ridge_model, newx = X_train)
residuals[[i]] <- Y_train - Y_pred_train
}
# Create a data frame for the residuals
residual_df <- data.frame(Lambda = log(lambda_seq), Residual = unlist(residuals))
# Create a histogram or density plot of residuals for different lambda values
library(ggplot2)
residual_plot <- ggplot(residual_df, aes(x = Residual, fill = Lambda)) +
geom_histogram(binwidth = 0.2, position = "identity") +
scale_fill_gradient(low = "blue", high = "red") +
labs(x = "Residual", y = "Frequency", fill = "Log Lambda (Regularization Strength)") + theme_minimal()
residual_plot
rm(list=ls())
library(plotly)
library(glmnet)
library(caret)
library(Metrics)
library(ISLR)
library(tidyr)
# Generate a synthetic dataset with multicollinearity and added noise
set.seed(123)
n <- 200
x1 <- rnorm(n)
x2 <- 2 * x1 + rnorm(n)
x3 <- -x1 + rnorm(n)
y <- 2 * x1 + 3 * x2 - x3 + rnorm(n)
#predictors
housing_data <- data.frame(X1 = x1, X2 = x2, X3 = x3, Y = y)
# standardize predictors
X_train <- scale(housing_data[, -4])
Y_train <- housing_data$Y
# Fit a Ridge Regression model with a sequence of lambda values
lambda_seq <- 10^seq(-2, 4, length = 100)
ridge_model <- glmnet(X_train, Y_train, alpha = 0, lambda = lambda_seq)
# Extract and transpose the coefficients and convert to a data frame
coef_matrix <- as.matrix(coef(ridge_model))
coef_df <- data.frame(lambda = log(lambda_seq), t(coef_matrix))
traces <- list()
# Loop predictor variable and create a trace for each
for (i in seq(nrow(coef_matrix))) {
if (i + 1 > ncol(coef_df)) {
stop("Error: Column index out of bounds.")
}
trace <- list(
x = coef_df$lambda,
y = coef_df[, i + 1],
type = "scatter",
mode = "lines",
name = colnames(coef_matrix)[i]
)
traces[[i]] <- trace
}
plot_ly(data = coef_df, x = ~lambda) %>%
add_trace(x = ~lambda, y = ~coef_df[,2], type = "scatter", mode = "lines", name = colnames(coef_df)[2]) %>%
add_trace(x = ~lambda, y = ~coef_df[,3], type = "scatter", mode = "lines", name = colnames(coef_df)[3]) %>%
add_trace(x = ~lambda, y = ~coef_df[,4], type = "scatter", mode = "lines", name = colnames(coef_df)[4]) %>%
# Add more traces for other coefficients as needed
layout(
title = "Ridge Regression Coefficients vs. Log Lambda",
xaxis = list(title = "Log Lambda (Regularization Strength)"),
yaxis = list(title = "Coefficient Value")
)
# condition number df for each lambda value
cond_df <- data.frame(lambda = lambda_seq, cond_num = NA)
# Loop through each lambda value and calculate the condition number
for (i in seq_along(lambda_seq)) {
lambda <- lambda_seq[i]
ridge_model <- glmnet(X_train, Y_train, alpha = 0, lambda = lambda)
X_mat <- as.matrix(X_train)  # Use the standardized predictor matrix directly
cond_num <- max(svd(X_mat)$d) / min(svd(X_mat)$d)
cond_df[i, "cond_num"] <- cond_num
}
# display the condition number
cond = ggplot(cond_df, aes(x = lambda, y = cond_num)) +
geom_line() + geom_point() +
labs(x = "Lambda", y = "Condition Number") + theme_minimal()
plot_ly(data = coef_df, x = ~lambda) %>%
add_trace(x = ~lambda, y = ~coef_df[,2], type = "scatter", mode = "lines", name = colnames(coef_df)[2]) %>%
add_trace(x = ~lambda, y = ~coef_df[,3], type = "scatter", mode = "lines", name = colnames(coef_df)[3]) %>%
add_trace(x = ~lambda, y = ~coef_df[,4], type = "scatter", mode = "lines", name = colnames(coef_df)[4]) %>%
layout(title = "Ridge Regression Coefficients vs. Log Lambda",
xaxis = list(title = "Log Lambda (Regularization Strength)"),
yaxis = list(title = "Coefficient Value")
) %>% subplot( ggplotly( ggplot(cond_df, aes(x = lambda, y = cond_num)) +geom_line() + geom_point() + labs(x = "Lambda", y = "Condition Number") + theme_minimal() ), nrows = 2)
# X - axis = lambda conveys the condition number.
# predictions on the training set
Y_pred_train <- predict(ridge_model, newx = X_train)
pred_df_train <- data.frame(actual = Y_train, predicted = Y_pred_train)
# predictions on the training set again
Y_pred_train <- predict(ridge_model, newx = X_train)
# Actual and predicted values
pred_df_train <- data.frame(actual = Y_train, predicted = Y_pred_train)
# display the actual versus predicted values
pred_plot <- ggplot(pred_df_train, aes(x = Y_train, y = Y_pred_train)) + geom_point() + geom_smooth(method = "lm", color = "red", se = FALSE) + labs(x = "Actual Value", y = "Predicted Value") + theme_minimal()
pred_plot
library(GGally)
predictor_df <- data.frame(X_train); predictor_df$Y_train <- Y_train
# scatter plot
scatter_plot <- ggpairs(predictor_df, columns = 1:seq(ncol(predictor_df)), aes(color = Y_train))
# subplot
subplot(scatter_plot, pred_plot, nrows = 2)
# residuals at different lambda
lambda_seq <- 10^seq(-2, 4, length = 100)
residuals <- vector("list", length(lambda_seq))
for (i in seq_along(lambda_seq)) {
lambda <- lambda_seq[i]
ridge_model <- glmnet(X_train, Y_train, alpha = 0, lambda = lambda)
Y_pred_train <- predict(ridge_model, newx = X_train)
residuals[[i]] <- Y_train - Y_pred_train
}
# residuals df
residual_df <- data.frame(Lambda = log(lambda_seq), Residual = unlist(residuals))
# histogram of lambda values
residual_plot <- ggplot(residual_df, aes(x = Residual, fill = Lambda)) +
geom_histogram(binwidth = 0.2, position = "identity") +
scale_fill_gradient(low = "blue", high = "red") +
labs(x = "Residual", y = "Frequency", fill = "Log Lambda (Regularization Strength)") + theme_minimal()
residual_plot
rm(list=ls())
getwd()
tinytex::tlmgr_update()
tinytex:::is_tinytex()
tinytex::tlmgr_self_update()
tinytex::reconfigure()
library(tinytex)
tinytex::tlmgr_update()
tinytex:::is_tinytex()
install.packages("tinytex")
install.packages("tinytex")
tinytex::install_tinytex()
tinytex:::use_tinytex()
tinytex:::use_tinytex()
ll
ls
data1 = read.table("CH01PR20.txt")
data2 = read.table("CH08PR15.txt")
rm(list = ls())
setwd("/Users/adamkurth/Documents/RStudio/STP 530/Homework 8/")
data1 = read.table("CH01PR20.txt")
data2 = read.table("CH08PR15.txt")
num.min = data1$V1
copiers = data1$V2
type = data2$V1
head(type)
m.R = lm(num.min ~ copiers  + type, data = newdata)
rm(list = ls())
setwd("/Users/adamkurth/Documents/RStudio/STP 530/Homework 8/")
data1 = read.table("CH01PR20.txt")
data2 = read.table("CH08PR15.txt")
num.min = data1$V1
copiers = data1$V2
type = data2$V1
newdata = data.frame(num.min, copiers, type)
m.F = lm(num.min ~ copiers  + type + copiers:type, data = newdata)
summary(m1)
m.R = lm(num.min ~ copiers  + type, data = newdata)
summary(m.R)
plot(m.R)
plot(m.F)
vif(m.F)
library(car)
vif(m.F)
vif(m.F)
vif(m.R)
newdata = data.frame(num.min, copiers, type)
m.F = lm(num.min ~ copiers  + type + copiers:type, data = newdata)
summary(m.F)
plot(m.F)
vif(m.F)
par(mfrow=c(2,2))
plot(m.R)
m.R = lm(num.min ~ copiers  + type, data = newdata)
summary(m.R)
par(mfrow=c(2,2))
plot(m.R)
vif(m.R)
# full model
par(mfrow=c(2,2))
plot(m.F)
vif(m.F)
anova(m.F, m.R)
summary(m.F)
1.824^2
summary(m.F)
summary(m.F)
anova(m.F, m.R)
anova(m.F, m.R)[1]
anova(m.F, m.R)[6]
anova(m.F, m.R)[4]
anova(m.F, m.R)[2,6]
pvalue = anova(m.F, m.R)[2,6]
pvalue
getwd()
setwd("/Users/adamkurth/Documents/vscode/CXFEL_Image_Analysis/CXFEL/unitcell_project//")
setwd("/Users/adamkurth/Documents/vscode/CXFEL_Image_Analysis/CXFEL/unitcell_project/")
